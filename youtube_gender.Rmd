---
title: "youtube_gender"
output: html_document
date: "2023-10-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#library("quanteda", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
#library("quanteda.textstats",quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library(quanteda)
library(quanteda.textstats)
library("stringi")
library("lubridate")
library(e1071)
```



```{r}
library("quanteda", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("quanteda.textstats",quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("quanteda.dictionaries", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("quanteda.textplots", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("stm", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("tidyverse", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("zoo", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library(ggplot2)
library(viridis)
```


```{r}
#my_data$my_variable <- log(my_data$my_variable)
#e$logcompound <- log(e$compound)
#hist(e$logcompound)
```

```{r}
library(tidyverse)
# Fit the OLS regression model
model1 <- lm(compound ~ gender, data = e)
model2 <- lm(compound ~ gender + Art, data = e)
model3 <- lm(compound ~ language + gender + Art, data = e)

# Summarize the regression results
summary(model1)
summary(model2)
summary(model3)
```
The "Intercept" is the estimated value of the dependent variable when all other independent variables are zero.
"language" has an estimated coefficient of approximately 0.121, which means that for each unit increase in the "language" variable (from 0 to 1), the dependent variable "compound" is expected to increase by 0.121 units.
"gender" has an estimated coefficient of approximately -0.075, which means that for each unit increase in the "gender" variable (from 0 to 1), the dependent variable "compound" is expected to decrease by 0.075 units.
"Art" is a categorical variable with multiple levels (chef, dancer, painter, and singer). Each level of "Art" is compared to the reference level (which is not shown in your output). For example, "Artdancer" has an estimated coefficient of approximately 0.074, which means that being an "Artdancer" is associated with an increase of 0.074 units in the "compound" compared to the reference level.
Significance: The p-values associated with each coefficient test whether the effect of each independent variable is statistically significant. In your case, all the coefficients, except "Artchef," have very low p-values (typically less than 0.05), indicating that they are statistically significant. This suggests that "language," "gender," and the different levels of "Art" are associated with the "compound" in a statistically significant way.

R-squared: The multiple R-squared value (0.0405) indicates the proportion of variance in the dependent variable "compound" explained by the independent variables. Adjusted R-squared (0.04047) adjusts the R-squared for the number of predictors in the model.

F-statistic: The F-statistic tests whether the overall model is statistically significant. In your case, the p-value for the F-statistic is very close to zero, indicating that the model is statistically significant.

Overall, it appears that the model is statistically significant, and the independent variables "language," "gender," and the levels of "Art" have significant associations with the dependent variable "compound."


In your case, with an Adjusted R-squared of 0.04047, it means that about 4.05% of the variance in the dependent variable "compound" is explained by the independent variables in your model. Whether this is considered "good" depends on the specific objectives of your analysis. In some fields, a high Adjusted R-squared might be desirable, while in others, such as social sciences, where human behavior is complex and difficult to predict, lower values can be typical.

In summary, the interpretation of a "good" Adjusted R-squared value is relative and should be made in the context of your research goals, the significance of the relationships, and comparisons to alternative models.

***
With a sample size of 189,000 comments from YouTube, you have a very large dataset. In the context of text sentiment analysis, it's common for the Adjusted R-squared value to be relatively low. Text sentiment analysis often involves many unique and complex text patterns, which can make it challenging to explain a large portion of the variance in sentiment scores using traditional regression models.

In this context, an Adjusted R-squared value of 0.04047 (4.05%) is not unexpected and may still be meaningful, especially if the relationships between your independent variables (language, gender, and art) and sentiment scores are theoretically and practically significant.

It's crucial to consider the specific goals and context of your analysis. In text sentiment analysis, the focus is often on understanding the influence of variables on sentiment, even if the overall model fit (Adjusted R-squared) is relatively low. Additionally, a low Adjusted R-squared value does not necessarily mean that the relationships you've identified are unimportant; it merely indicates that they explain a relatively small proportion of the variance in sentiment scores.

***

The regression results you've provided suggest that you have a model with sentiment score (compound) as the dependent variable and three independent variables: language, gender, and Art (with specific categories within Art, such as Artchef, Artdancer, Artpainter, and Artsinger).

Here's an interpretation of the key findings:

Coefficients: The coefficients in the output show the estimated effect of each independent variable on the sentiment score (compound).

The Intercept (constant) is 0.274966.
For each unit increase in the 'language' variable, there is an estimated increase of 0.120722 in the sentiment score.
For each unit increase in the 'gender' variable, there is an estimated decrease of 0.074962 in the sentiment score.
The 'Art' variable is categorical, with multiple categories (Artchef, Artdancer, Artpainter, and Artsinger). The coefficients for these categories show the estimated differences in sentiment score compared to the baseline category (which is not explicitly shown).
Significance: The 't value' and 'Pr(>|t|)' (p-value) columns provide information about the statistical significance of each coefficient. In your results, all the coefficients are highly statistically significant because the p-values are very close to zero (much less than the standard significance level of 0.05).

R-squared: The R-squared value is a measure of the goodness of fit of the model. In your model, the multiple R-squared is 0.0405, which means that the independent variables explain about 4.05% of the variance in the dependent variable (sentiment score). This is a relatively low value, suggesting that the model doesn't explain a large proportion of the variability in sentiment scores.

Adjusted R-squared: The adjusted R-squared is a modification of the R-squared value that accounts for the number of independent variables in the model. It is often preferred when comparing models with different numbers of variables. In your case, the adjusted R-squared is very close to the multiple R-squared, which implies that adding the independent variables didn't significantly improve the model's explanatory power.

Residual Standard Error: The residual standard error (0.3598) indicates the average magnitude of the residuals (the differences between the observed and predicted values). Smaller values of this statistic indicate a better fit of the model to the data.

Overall, your model indicates that language and gender have statistically significant effects on sentiment scores, while the specific categories within the 'Art' variable also have statistically significant effects compared to the baseline category. However, the model's low R-squared values suggest that there are other factors or unexplained variability in the sentiment scores that are not accounted for by this model. You may want to consider exploring additional variables or alternative models to improve the model's explanatory power.


```{r}
library(car)
#vif(model1)
vif(model2)
vif(model3)
```

```{r}
library(lmtest)
# BPG test for heteroscedasticity
bptest(model1)
bptest(model2)
bptest(model3)
```

```{r}
#resettest(model1, power =2:3, type ="regressor", data = e)
#resettest(model2, type ="regressor", data = e)
#resettest(model3, power =2:3, type ="regressor", data = e)
```

```{r}
# interaction between gender and language
md <- lm(formula = compound ~ language * gender + Art, data = e)
summary(md)
```
Interaction between 'language' and 'gender':
This interaction term captures whether the relationship between language and sentiment score differs for different genders. It accounts for the possibility that the effect of language on sentiment may be different for males and females.

```{r}
library("quanteda", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library("topicmodels", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)
library(dplyr)
library(lubridate)
library(ggplot2)
library("stm")
library("tidyverse")
library("stringi")
library("lubridate")
library(e1071)
```

```{r}
combined <- read.csv('FINAL2.csv', stringsAsFactors = F)

# making a Year column
combined$Year <- format(as.Date(combined$date), "%Y")
comment_counts <- table(combined$Year)
# using 'dmy' to specify the date format
combined$date <- dmy(combined$date)  
combined$day_of_year <- yday(combined$date)
corpus_combined <- corpus(combined$Translated_Comments,
                       docvars = combined[,c("date", "day_of_year", "Year",
                                       "gender", "Art")])
# transforming gender and party into factors
combined$gender <- ifelse(combined$gender == 0, "Male", "Female")
combined$Art <- factor(combined$Art) # factoring is necessary for multiple category variables

# Tokenisation 
toks_com <- tokens(corpus_combined, remove_punct = TRUE, remove_symbols = TRUE, 
               remove_numbers = TRUE, remove_url = TRUE) %>%
        tokens_tolower() %>%
        tokens_remove(c(stopwords(source = "smart"), 'apnar', 'r', 'khub', 'apu', 'ei', 'e', 'a_href', 'ahm', 'na', 'ki', 'er', 'i', 'na', 's', 'href', 'amr', 'muza', 'xefer', 'm', 'a', 'deepanita', 'ai', 'm', 'chai', 'ridy', 'amr', 'bihu', 'ridi', 'farjana', 'danc', 'latif', 'curri', 've', 'hoy', 'ur', 'ar', 'pls', 'plz', 'onk', 'liza', 'bt', '#name', 'nusrat', 'umm', 'safa', 'manna', 'appl', 'valo','amar', 'onek' )) %>% tokens_replace(pattern = "[\U0001F600-\U0001F6FF]", replacement = "", valuetype = "regex") %>% tokens_wordstem()

# Document Feature Matrix
dfm_com <- dfm(toks_com) %>% 
       dfm_trim(min_termfreq = 20) # to remove features (terms) that have a term frequency (i.e., the number of times a term appears in the corpus) less than or equal to 20. This step is often used to reduce the dimensionality of the DFM and focus on the most relevant or frequently occurring terms in your text data.

#Adding the gender column to the dataframe:
#dfm_com$gender <- combined$gender
```

```{r}
#tstat_lexdiv_TRR <- textstat_lexdiv(dfm_com)
tstat_lexdiv_TRR <- textstat_lexdiv(dfm_com, measure=c("TTR"))
# Reference: https://stackoverflow.com/questions/19697498/mean-warning-argument-is-not-numeric-or-logical-returning-na

mean(tstat_lexdiv_TRR$TTR, na.rm = TRUE)
sd(tstat_lexdiv_TRR$TTR, na.rm = TRUE)
range(tstat_lexdiv_TRR$TTR, na.rm = TRUE)
skewness(tstat_lexdiv_TRR$TTR, na.rm = TRUE)
kurtosis(tstat_lexdiv_TRR$TTR, na.rm = TRUE)
```
Approximately 95.8% of the words used are unique or different from each other, while the remaining 4.2% are repetitions or instances of the same word. A higher TTR generally suggests a richer and more diverse vocabulary in the text, while a lower TTR indicates a more repetitive or limited vocabulary.
```{r}
tstat_lexdiv_R <- textstat_lexdiv(dfm_com, measure=c("R"))
mean(tstat_lexdiv_R$R, na.rm = TRUE)
sd(tstat_lexdiv_R$R, na.rm = TRUE)
range(tstat_lexdiv_R$R, na.rm = TRUE)
skewness(tstat_lexdiv_R$R, na.rm = TRUE)
kurtosis(tstat_lexdiv_R$R, na.rm = TRUE)
```

```{r}
tstat_lexdiv_D <- textstat_lexdiv(dfm_com, measure=c("D"))
mean(tstat_lexdiv_D$D, na.rm = TRUE)
sd(tstat_lexdiv_D$D, na.rm = TRUE)
range(tstat_lexdiv_D$D, na.rm = TRUE)
skewness(tstat_lexdiv_D$D, na.rm = TRUE)
kurtosis(tstat_lexdiv_D$D, na.rm = TRUE)
```

```{r}
tstat_lexdiv_TRR <- textstat_lexdiv(dfm_com, measure=c("TTR", "R", "D"))
```




```{r}
tstat_lexdiv_avg <- textstat_lexdiv(tokens(data_corpus_inaugural), measure="MATTR")
tstat_lexdiv_avg <- textstat_lexdiv(toks_com, measure="MATTR")
mean(tstat_lexdiv_avg$MATTR, na.rm = TRUE)
sd(tstat_lexdiv_avg$MATTR, na.rm = TRUE)
range(tstat_lexdiv_avg$MATTR, na.rm = TRUE)
skewness(tstat_lexdiv_avg_D$MATTR, na.rm = TRUE)
kurtosis(tstat_lexdiv_avg$MATTR, na.rm = TRUE)

```


```{r}

```


# Keyness Analysis
```{r fig1, fig.height = 3, fig.width = 5}
# Keyness Analysis for female corpus:
keyness_posts <- textstat_keyness(dfm_com, target = dfm_com$gender == 1)
keyness_posts1 <- textstat_keyness(dfm_com, target = dfm_com$gender == 0)
textplot_keyness(keyness_posts)
```



```{r fig1, fig.height = 3, fig.width = 3}
#kwic(corpus_combined, "beauty", window=10)[1:5,]
textplot_wordcloud(keyness_posts1, rotation = 0.25, 
                   color = rev(RColorBrewer::brewer.pal(8, "RdBu")),
                   max_words = 100)
```

```{r fig1, fig.height = 3, fig.width = 3}
textplot_wordcloud(keyness_posts, rotation = 0.35, 
                   color = rev(RColorBrewer::brewer.pal(8, "RdBu")),
                   max_words = 100)
```

```{r}
# splitting the dfm by gender:
gender_docvar <- docvars(dfm_com)$gender
dfm_gender_0 <- dfm_com[gender_docvar == 0, ]
dfm_gender_1 <- dfm_com[gender_docvar == 1, ]
```


```{r}
# converting the dfm object to stm object:
stm_input <- convert(dfm_com, to = "stm")
```

# structural topic modeling: 
```{r}
set.seed(321)
stmodel1 <- stm(documents = stm_input$documents, 
                vocab = stm_input$vocab,
                K = 10, 
                prevalence =~ gender + s(day_of_year),
               #Hypothesis: We expect that gender and day of the year have effect on discussion of certain topics.. 
               data = stm_input$meta, 
               verbose = FALSE)
```


```{r, fig1, fig.height = 2, fig.width = 3}
plot(stmodel1)
```

```{r}
cloud(stmodel1, topic = 1, scale = c(5,.25)) # food or recipe
cloud(stmodel1, topic = 2, scale = c(5,.25))
cloud(stmodel1, topic = 3, scale = c(5,.25))
cloud(stmodel1, topic = 4, scale = c(5,.25))
cloud(stmodel1, topic = 5, scale = c(5,.25))
cloud(stmodel1, topic = 6, scale = c(5,.25))
cloud(stmodel1, topic = 7, scale = c(5,.25))
cloud(stmodel1, topic = 8, scale = c(5,.25))
cloud(stmodel1, topic = 9, scale = c(5,.25))
cloud(stmodel1, topic = 10, scale = c(5,.25))
```

```{r}
cloud(stmodel1, topic = 3, scale = c(5,.25))
```

```{r}
toLDAvis(stmodel1, docs = stm_input$documents)
```



```{r}
effect_estimates <- estimateEffect(1:10 ~ gender + s(day_of_year), stmodel1, meta = stm_input$meta)
```

```{r}
plot(effect_estimates, covariate = "gender", topics = c(1,9,4,10 ),
     model = stmodel1, method = "difference",
     cov.value1 = "Male", cov.value2 = "Female",
     xlab = "More Male ... More Female", 
     main = "Male and Female",
     xlim = c(-.1, .1), labeltype = "custom", 
     custom.labels = c("appearance", "encouragement", "music", "recipe"))
```


```{r}
toLDAvis(stmodel1, docs = stm_input$documents)
```









```{r}
comments <- comment_corpus %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,
         remove_url = TRUE) %>%
        tokens_remove(c('apnar', 'r', 'khub', 'apu', 'ei', 'e', 'a_href', 'ahm', 'na', 'ki', 'er', 'i', 'na', 's', 'href', 'amr', 'muza', 'xefer', 'm', 'a', 'deepanita', 'ai', 'm', 'chai', 'ridy')) %>% tokens_ngrams(n = 1:2) %>% dfm() %>% dfm_trim(min_termfreq = 5)
comments
```

```{r}
dfm_com <- dfm(comments) %>% 
       dfm_trim(min_termfreq = 20)
```




```{r fig1, fig.height = 5, fig.width = 5}
textplot_wordcloud(dfm_com, rotation = 0.25, 
                   color = rev(RColorBrewer::brewer.pal(8, "RdBu")),
                   max_words = 300)
```

```{r}
dfm_com$gender <- e$gender
```



```{r}
# Keyness Analysis for female corpus:
#keyness_posts <- textstat_keyness(comments_dfm, target = comments_dfm$gender == 0)
#textplot_keyness(keyness_posts)
# Keyness Analysis for male corpus:
keyness_posts <- textstat_keyness(dfm_com, target = comments_dfm$gender == 1)
textplot_keyness(keyness_posts)
```





```{r}
stm_input <- convert(comments_dfm, to = "stm")
```


```{r}
stm_input$meta
```


```{r}
stmodel2 <- stm(documents = stm_input$documents, 
                vocab = stm_input$vocab,
                K = 30, 
                prevalence =~ gender,
               #Hypothesis: We expect that party and day of the year have effect on discussion of certain topics. Note that we don't have dependent variable here. 
               data = stm_input$meta, 
               verbose = FALSE)
```

```{r fig1, fig.height = 3, fig.width = 4}
plot(stmodel2)
```







```{r}
# Corpus
corpus_e <- corpus(e, text_field = "Translated_Comments")

# Number of tokens and types
ntoken(corpus_e) %>% sum()
ntype(corpus_e) %>% sum()
```

```{r}
# Male corpus
corpus_m_e<- corpus_e[corpus_e$gender == 0, ]
# Tokenisation 
corpus_m_e <- tokens(corpus_m_e) 
toks_m <- tokens(corpus_m_e, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>% 
        tokens_remove(c(stopwords(source = "smart"), '#39', 'ur')) %>% 
        tokens_wordstem()


# Document Feature Matrix
dfm_m <- dfm(toks_m) %>% 
       dfm_trim(min_termfreq = 2)
textplot_wordcloud(dfm_m, rotation = 0.35, 
                   color = rev(RColorBrewer::brewer.pal(8, "RdBu")),
                   max_words = 200)
```

```{r}
corpus_f_e <- corpus_e[corpus_e$gender == 1, ]
# Tokenisation 
corpus_f_e <- tokens(corpus_f_e) 
toks_f <- tokens(corpus_f_e, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>% 
        tokens_remove(c(stopwords(source = "smart"), '#39', 'ur')) %>% 
        tokens_wordstem()



# Document Feature Matrix
dfm_f <- dfm(toks_f) %>% 
       dfm_trim(min_termfreq = 2)
textplot_wordcloud(dfm_f, rotation = 0.35, 
                   color = rev(RColorBrewer::brewer.pal(8, "RdBu")),
                   max_words = 200)
```



```{r}
# Top features
top_words <- tokens(corpus_e, remove_punct = TRUE, remove_symbols = TRUE,
                    remove_numbers = TRUE) %>% 
             tokens_tolower() %>% dfm()
top_features <- as.data.frame(topfeatures(top_words, 500)) 
top_features <- top_features %>% 
                mutate(count = `topfeatures(top_words, 500)`,
                       word = rownames(top_features))

# Plotting
word.plot <- ggplot(data = top_features, 
                    aes(x = 1:500, y = log(count), label = word)) +
             geom_line() + 
             geom_text(size = 2.5, hjust = 0, vjust = 0, nudge_x = 0.5) +
             labs(x = "word", y = "log(count)") + theme_classic()
```

```{r}
word.plot
```

Zipf’s Law is a statistical distribution in certain data sets, such as words in a linguistic corpus, in which the frequencies of certain words are inversely proportional to their ranks. Named for linguist George Kingsley Zipf, who around 1935 was the first to draw attention to this phenomenon, the law examines the frequency of words in natural language and how the most common word occurs twice as often as the second most frequent word, three times as often as the subsequent word and so on until the least frequent word. The word in the position n appears 1/n times as often as the most frequent one.

When words are ranked according to their frequencies in a large enough collection of texts and then the frequency is plotted against the rank, the result is a logarithmic curve. (Or if you graph on a log scale, the result is a straight line.)

The most common word in English is “the,” which appears about one-tenth of the time in a typical text; the next most common word (rank 2) is “of,” which appears about one-twentieth of the time. In this type of distribution, frequency declines sharply as the rank number increases, so a small number of items appear very often, and a large number rarely occur.

A Zipfian distribution of words is universal in natural language: It can be found in the speech of children less than 32 months old as well as in the specialized vocabulary of university textbooks. Studies show that this phenomenon also applies in nearly every language.



```{r}
# define a function to remove emojis
remove_emoji <- function(x) {
  gsub(pattern = "[\U0001F600-\U0001F6FF]", "", x)
}

# apply the function to your text data
corpus_f_e <- corpus_f_e %>%
  tokens(remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords(source = "smart")) %>% 
  tokens_wordstem() %>%
  tokens_replace(pattern = "[\U0001F600-\U0001F6FF]", replacement = "", valuetype = "regex")

# Tokenisation 
toks_f <- tokens(corpus_f_e, remove_punct = TRUE, remove_symbols = TRUE, 
               remove_numbers = TRUE, remove_url = TRUE) %>%
        tokens_tolower() %>%
        tokens_remove(c(stopwords(source = "smart"), '#39', 'ur', 'apnar')) %>% 
        tokens_wordstem()

# Document Feature Matrix
dfm_f <- dfm(toks_f) %>% 
       dfm_trim(min_termfreq = 20)

```


## topic modeling

```{r}
# converting the data frame into stm object, 
stm_f <- convert(dfm_f, to = "stm")
stm_f
```

```{r}      
stmodel_f <- stm(documents = stm_f$documents, vocab = stm_f$vocab,
                     K = 20, prevalence =~ gender,
               #Hypothesis: We expect that party and day of the year have effect on discussion of certain topics. Note that we don't have dependent variable here. 
               data = stm_f$meta, verbose = FALSE)
?stm
```


```{r}
plot(stmodel_f)
```

```{r}
cloud(stmodel_f, topic = 2, scale = c(2,.25)) #appearance
cloud(stmodel_f, topic = 6, scale = c(2,.25)) #encouragement
cloud(stmodel_f, topic = 8, scale = c(2,.25)) #food
```
```{r}
effect_estimates1 <- estimateEffect(1:10 ~ gender, stmodel_f, meta = stm_f$meta)
```

```{r}
plot(effect_estimates1, covariate = "gender", topics = c(2, 6, 8),
     model = stmodel_f, method = "difference",
     cov.value1 = "male", cov.value2 = "female",
     xlab = "More female ... More male", 
     main = "Democrats and Republicans",
     xlim = c(-.1, .1), labeltype = "custom", 
     custom.labels = c("appearance", "Encouragement",
                       "Food"))
```

